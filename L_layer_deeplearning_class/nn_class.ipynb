{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e59decb9-b964-417c-8cdb-3fb7037def3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries being used:\n",
    "import numpy as np\n",
    "\n",
    "#Loading bar\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a582631-00ec-4fae-9964-fd7909a392da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(Z):\n",
    "\n",
    "    return (Z > 0).astype(float)\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "    \n",
    "    S = 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    return S * (1 - S)\n",
    "\n",
    "def entropy_loss_function_derivative(AL, Y):\n",
    "    \n",
    "    dAL = - (Y / AL) + ((1 - Y) / (1 - AL))\n",
    "\n",
    "    return dAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cf9e1d29-0cbe-48b5-b35a-f69cf699726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L_layer_NN:\n",
    "\n",
    "    #Later: add mini-batch\n",
    "    #add dropout for regularization with all optimizers\n",
    "    #add Clipping, but first understand where clipping is required and why and how it fixes it and what it is\n",
    "\n",
    "    def __init__(self, layer_dims, learning_rate=0.01, lambd = 0, optimizer = \"gd\", initialization = \"he\", beta1 = 0.9, beta2 = 0.999, batch_size = 64):\n",
    "\n",
    "        self.layer_dims = layer_dims\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambd = lambd\n",
    "        self.optimizer = optimizer.lower()\n",
    "        self.initialization = initialization\n",
    "        self.parameters = self._initialize_parameters()\n",
    "        self.costs = []\n",
    "        self.grads = {}\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epoch = 1\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "\n",
    "        self._initialize_optimizer()\n",
    "    \n",
    "    def _initialize_parameters(self):\n",
    "\n",
    "        parameters = {}\n",
    "\n",
    "        L = len(self.layer_dims)\n",
    "    \n",
    "        for l in range(1, L):\n",
    "\n",
    "            #He initialization\n",
    "            if(self.initialization == \"he\"): \n",
    "                parameters['W' + str(l)] = np.random.randn(self.layer_dims[l], self.layer_dims[l-1]) * np.sqrt(2.0 / self.layer_dims[l-1])\n",
    "                parameters['b' + str(l)] = np.zeros((self.layer_dims[l], 1))\n",
    "\n",
    "            #random initialization \n",
    "            elif(self.initialization == \"random\"):\n",
    "                parameters['W' + str(l)] = np.random.randn(self.layer_dims[l], self.layer_dims[l-1]) * 0.01\n",
    "                parameters['b' + str(l)] = np.zeros((self.layer_dims[l], 1))\n",
    "\n",
    "            #throw error\n",
    "            else:\n",
    "                raise ValueError(f'Unknown parameter initialization : \"{self.initialization }\" | please try: he, random, etc')\n",
    "\n",
    "        return parameters\n",
    "\n",
    "        \n",
    "\n",
    "    def _linear_forward(self, A, W, b):\n",
    "\n",
    "        Z = W @ A + b\n",
    "\n",
    "        #cache used for _linear_backward\n",
    "        linear_cache = (A, W, b)\n",
    "\n",
    "        return Z, linear_cache\n",
    "\n",
    "        \n",
    "\n",
    "    def _linear_activation_forward(self, A_prev, W, b, activation):\n",
    "\n",
    "        Z, linear_cache = self._linear_forward(A_prev, W, b)\n",
    "        #cache used for activation _linear_activation_backward\n",
    "        activation_cache = Z\n",
    "\n",
    "        if activation == \"relu\":\n",
    "            A = np.maximum(0, Z)\n",
    "        elif activation == \"sigmoid\":\n",
    "            A = 1 / (1 + np.exp(-Z))\n",
    "        elif activation == \"linear\": \n",
    "            A = Z\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\") \n",
    "\n",
    "        cache = (linear_cache, activation_cache)\n",
    "\n",
    "        return A, cache\n",
    "\n",
    "        \n",
    "\n",
    "    def _forward(self, X):\n",
    "        \n",
    "        A = X\n",
    "        L = len(self.layer_dims) - 1\n",
    "        caches = []\n",
    "    \n",
    "        for l in range(1, L):\n",
    "    \n",
    "            W = self.parameters[\"W\" + str(l)]\n",
    "            b = self.parameters[\"b\" + str(l)]\n",
    "    \n",
    "            A, cache = self._linear_activation_forward(A, W, b, \"relu\")\n",
    "            caches.append(cache)\n",
    "    \n",
    "        W = self.parameters[\"W\" + str(L)]\n",
    "        b = self.parameters[\"b\" + str(L)]\n",
    "    \n",
    "        AL, cache = self._linear_activation_forward(A, W, b, \"sigmoid\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "        return AL, caches\n",
    "\n",
    "    def _compute_cost(self, AL, Y):\n",
    "\n",
    "        m = Y.shape[1]\n",
    "        #Cost-entropy cost function:\n",
    "        eps = 1e-8\n",
    "        AL_clipped = np.clip(AL, eps, 1 - eps)\n",
    "        cost = -(1 / m) * (Y @ np.log(AL_clipped).T + (1 - Y) @ np.log(1 - AL_clipped).T)\n",
    "\n",
    "        #L2 regularization\n",
    "        L2_term = 0 \n",
    "        for key in self.parameters:\n",
    "            if key.startswith(\"W\"):\n",
    "                L2_term += np.sum(np.square(self.parameters[key])) \n",
    "\n",
    "        #add l2 reg into cost\n",
    "        cost += (self.lambd / (2 * m)) * L2_term\n",
    "\n",
    "        assert cost.shape == (1, 1)\n",
    "        \n",
    "        return np.squeeze(cost)\n",
    "\n",
    "    def _linear_backward(self, dZ, cache):\n",
    "        \n",
    "        A_prev, W, b = cache\n",
    "        \n",
    "        m = A_prev.shape[1]\n",
    "        \n",
    "        dW = (dZ @ A_prev.T) / m \n",
    "        \n",
    "        db = np.sum(dZ, axis = 1, keepdims=True) / m\n",
    "        \n",
    "        dA_prev = W.T @ dZ\n",
    "    \n",
    "        return dA_prev, dW, db\n",
    "\n",
    "    def _linear_activation_backward(self, dA, cache, activation):\n",
    "\n",
    "        linear_cache, Z = cache\n",
    "\n",
    "        if activation == \"relu\":\n",
    "    \n",
    "            dZ = dA * relu_derivative(Z)\n",
    "    \n",
    "        elif activation == \"sigmoid\":\n",
    "    \n",
    "            dZ = dA * sigmoid_derivative(Z)\n",
    "    \n",
    "        elif activation == \"linear\":\n",
    "    \n",
    "            dZ = dA\n",
    "    \n",
    "        else: raise ValueError(f\"Unknown activation {activation}\")\n",
    "    \n",
    "        dA_prev, dW, db = self._linear_backward(dZ, linear_cache)\n",
    "    \n",
    "        return dA_prev, dW, db\n",
    "\n",
    "    def _backward(self, AL, Y, caches):\n",
    "        \n",
    "        L = len(self.layer_dims) - 1\n",
    "        \n",
    "        dAL = entropy_loss_function_derivative(AL, Y)\n",
    "        dA = dAL\n",
    "    \n",
    "        current_cache = caches[L - 1]\n",
    "    \n",
    "        dA_prev_temp, dW_temp, db_temp = self._linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "         \n",
    "        self.grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
    "        self.grads[\"dW\" + str(L)] = dW_temp\n",
    "        self.grads[\"db\" + str(L)] = db_temp\n",
    "    \n",
    "        for l in reversed(range(L - 1)):\n",
    "    \n",
    "            current_cache = caches[l]\n",
    "    \n",
    "            dA_prev_temp, dW_temp, db_temp = self._linear_activation_backward(dA_prev_temp, current_cache, \"relu\" )\n",
    "     \n",
    "            self.grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "            self.grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "            self.grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    def _initialize_optimizer(self):\n",
    "\n",
    "        if self.optimizer == \"momentum\":\n",
    "\n",
    "            self.velocities = {}\n",
    "            L = len(self.layer_dims) - 1\n",
    "            for l in range(L):\n",
    "    \n",
    "                self.velocities[\"vdW\" + str(l+1)] = np.zeros_like(self.parameters[\"W\" + str(l+1)])\n",
    "                self.velocities[\"vdb\" + str(l+1)] = np.zeros_like(self.parameters[\"b\" + str(l+1)])\n",
    "\n",
    "        if self.optimizer == \"rmsprop\":\n",
    "\n",
    "            self.squares = {}\n",
    "            L = len(self.layer_dims) - 1\n",
    "            for l in range(L):\n",
    "    \n",
    "                self.squares[\"sdW\" + str(l+1)] = np.zeros_like(self.parameters[\"W\" + str(l+1)])\n",
    "                self.squares[\"sdb\" + str(l+1)] = np.zeros_like(self.parameters[\"b\" + str(l+1)])\n",
    "\n",
    "        if self.optimizer == \"adam\":\n",
    "\n",
    "            self.velocities = {}\n",
    "            self.squares = {}\n",
    "\n",
    "            L = len(self.layer_dims) - 1\n",
    "            \n",
    "            for l in range(L):\n",
    "    \n",
    "                self.velocities[\"vdW\" + str(l+1)] = np.zeros_like(self.parameters[\"W\" + str(l+1)])\n",
    "                self.velocities[\"vdb\" + str(l+1)] = np.zeros_like(self.parameters[\"b\" + str(l+1)])\n",
    "\n",
    "                self.squares[\"sdW\" + str(l+1)] = np.zeros_like(self.parameters[\"W\" + str(l+1)])\n",
    "                self.squares[\"sdb\" + str(l+1)] = np.zeros_like(self.parameters[\"b\" + str(l+1)])\n",
    "                \n",
    "\n",
    "    def _update_gd(self):\n",
    "\n",
    "        L = len(self.layer_dims) - 1\n",
    "        m = self.grads[\"dA\" + str(L - 1)].shape[1]\n",
    "        \n",
    "        for l in range(L):\n",
    "\n",
    "            _dW = self.grads[\"dW\" + str(l+1)] + (self.lambd / m) * self.parameters[\"W\" + str(l + 1)] # -> dW with L2 reg\n",
    "            self.parameters[\"W\" + str(l+1)] -= self.learning_rate * _dW\n",
    "            self.parameters[\"b\" + str(l+1)] -= self.learning_rate * self.grads[\"db\" + str(l+1)]\n",
    "\n",
    "\n",
    "    def _update_momentum(self):\n",
    "\n",
    "        L = len(self.layer_dims) - 1\n",
    "        m = self.grads[\"dA\" + str(L - 1)].shape[1]\n",
    "\n",
    "        for l in range(L):\n",
    "\n",
    "            _dW = self.grads[\"dW\" + str(l+1)] + (self.lambd / m) * self.parameters[\"W\" + str(l + 1)] # -> dW with L2 reg\n",
    "\n",
    "            #update velocities:\n",
    "            self.velocities[\"vdW\" + str(l+1)] = self.beta1 * self.velocities[\"vdW\" + str(l+1)] + (1-self.beta1) * _dW\n",
    "            self.velocities[\"vdb\" + str(l+1)] = self.beta1 * self.velocities[\"vdb\" + str(l+1)] + (1-self.beta1) * self.grads[\"db\" + str(l+1)]\n",
    "            \n",
    "            #update parameters\n",
    "            self.parameters[\"W\" + str(l+1)] -= self.learning_rate * self.velocities[\"vdW\" + str(l+1)]\n",
    "            self.parameters[\"b\" + str(l+1)] -= self.learning_rate * self.velocities[\"vdb\" + str(l+1)]\n",
    "\n",
    "    def _update_rmsprop(self):\n",
    "\n",
    "        L = len(self.layer_dims) - 1\n",
    "        m = self.grads[\"dA\" + str(L - 1)].shape[1]\n",
    "\n",
    "        for l in range(L):\n",
    "\n",
    "            _dW = self.grads[\"dW\" + str(l+1)] # -> dW without L2 reg\n",
    "        \n",
    "            #update squares:\n",
    "            self.squares[\"sdW\" + str(l+1)] = self.beta2 * self.squares[\"sdW\" + str(l+1)] + (1-self.beta2) * _dW**2\n",
    "            self.squares[\"sdb\" + str(l+1)] = self.beta2 * self.squares[\"sdb\" + str(l+1)] + (1-self.beta2) * self.grads[\"db\" + str(l+1)]**2\n",
    "            \n",
    "            #update parameters\n",
    "            eps = 1e-8\n",
    "            self.parameters[\"W\" + str(l+1)] -= self.learning_rate * _dW / (np.sqrt( self.squares[\"sdW\" + str(l+1)]) + eps) \n",
    "            self.parameters[\"b\" + str(l+1)] -= self.learning_rate * self.grads[\"db\" + str(l+1)] / (np.sqrt( self.squares[\"sdb\" + str(l+1)]) + eps) \n",
    "\n",
    "\n",
    "    def _update_adam(self):\n",
    "\n",
    "        L = len(self.layer_dims) - 1\n",
    "        m = self.grads[\"dA\" + str(L - 1)].shape[1]\n",
    "\n",
    "        for l in range(L):\n",
    "\n",
    "            _dW = self.grads[\"dW\" + str(l+1)] # -> dW without L2 reg\n",
    "\n",
    "            #update velocities:\n",
    "            self.velocities[\"vdW\" + str(l+1)] = self.beta1 * self.velocities[\"vdW\" + str(l+1)] + (1-self.beta1) * _dW\n",
    "            self.velocities[\"vdb\" + str(l+1)] = self.beta1 * self.velocities[\"vdb\" + str(l+1)] + (1-self.beta1) * self.grads[\"db\" + str(l+1)]\n",
    "\n",
    "            #update squares:\n",
    "            self.squares[\"sdW\" + str(l+1)] = self.beta2 * self.squares[\"sdW\" + str(l+1)] + (1-self.beta2) * _dW**2\n",
    "            self.squares[\"sdb\" + str(l+1)] = self.beta2 * self.squares[\"sdb\" + str(l+1)] + (1-self.beta2) * self.grads[\"db\" + str(l+1)]**2\n",
    "\n",
    "            #bias correction for velocities:\n",
    "            _vdW_corrected = self.velocities[\"vdW\" + str(l+1)] / (1 - self.beta1**self.epoch)\n",
    "            _vdb_corrected = self.velocities[\"vdb\" + str(l+1)] / (1 - self.beta1**self.epoch)\n",
    "\n",
    "            #bias correction for squares:\n",
    "            _sdW_corrected = self.squares[\"sdW\" + str(l+1)] / (1 - self.beta2**self.epoch)\n",
    "            _sdb_corrected = self.squares[\"sdb\" + str(l+1)] / (1 - self.beta2**self.epoch)\n",
    "\n",
    "            #update parameters:\n",
    "            eps = 1e-8\n",
    "            self.parameters[\"W\" + str(l+1)] -= self.learning_rate * _vdW_corrected / ( np.sqrt(_sdW_corrected) + eps ) \n",
    "            self.parameters[\"b\" + str(l+1)] -= self.learning_rate * _vdb_corrected / ( np.sqrt(_sdb_corrected) + eps )\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _update_parameters(self):\n",
    "\n",
    "        if self.optimizer == \"gd\": self._update_gd()\n",
    "        elif self.optimizer == \"momentum\": self._update_momentum()\n",
    "        elif self.optimizer == \"rmsprop\": self._update_rmsprop()\n",
    "        elif self.optimizer == \"adam\": self._update_adam()\n",
    "        else: raise ValueError(f\"Unknown optimizer: {self.optimizer}\")\n",
    "        \n",
    "    \n",
    "    def _grad_check(self):\n",
    "        #add other values to be gotten by function\n",
    "        pass\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "\n",
    "        predictions = self.predict(X)\n",
    "        accuracy = np.mean(predictions == y)\n",
    "\n",
    "        return (accuracy * 100)\n",
    "\n",
    "    def _suffle_into_mini_batches(self, X, y):\n",
    "\n",
    "        m = X.shape[1]\n",
    "\n",
    "        #suffle data\n",
    "        permutation = np.random.permutation(X.shape[1])\n",
    "        X_shuffled = X[:, permutation]\n",
    "        y_shuffled = y[:, permutation]\n",
    "\n",
    "        mini_batches = []\n",
    "\n",
    "        for i in range(0, m, self.batch_size):\n",
    "\n",
    "            X_batch = X_shuffled[:, i : i + self.batch_size]\n",
    "            y_batch = y_shuffled[:, i : i + self.batch_size]\n",
    "\n",
    "            batch = np.vstack((X_batch, y_batch)) #shape (n_x + n_y, batch_size)\n",
    "            mini_batches.append(batch)\n",
    "\n",
    "        return mini_batches\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def train(self, X, y, epochs = 3000, print_cost=True):\n",
    "\n",
    "        n = X.shape[0]\n",
    "\n",
    "        progress = tqdm(range(epochs), desc=\"Training\", leave=True)\n",
    "\n",
    "        for i in progress:\n",
    "\n",
    "            mini_batches = self._suffle_into_mini_batches(X, y) #array with mini batchs, each batch shape: (n_x + n_y, batch_size) or (n_x + 1, batch_size)\n",
    "            epoch_cost = []\n",
    "            for batch in mini_batches:\n",
    "\n",
    "                batch_X = batch[:n, :]\n",
    "                batch_y = batch[n: , :]\n",
    "                #cjange the computation of cost to compute every #epochs or however design decision i want\n",
    "                AL, caches = self._forward(batch_X)\n",
    "                cost = self._compute_cost(AL, batch_y)\n",
    "    \n",
    "                self._backward(AL, batch_y, caches)\n",
    "                self._update_parameters()\n",
    "                epoch_cost.append(cost)\n",
    "\n",
    "            mean_cost = np.mean(epoch_cost)\n",
    "    \n",
    "            if i % 10 == 0:\n",
    "                self.costs.append((i, mean_cost))\n",
    "                if(print_cost): progress.set_postfix({ \"Mean cost\": f\"{mean_cost}\" })\n",
    "\n",
    "            self.epoch += 1\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        AL, _ = self._forward(X)\n",
    "\n",
    "        return (AL > 0.5).astype(int)\n",
    "\n",
    "    def plot_cost(self):\n",
    "\n",
    "        iters, values = zip(*self.costs)\n",
    "        plt.plot(iters, values)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Cost\")\n",
    "        plt.title(\"Cost vs Epoch\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8232537a-afe5-45f0-83f3-573ea7f684c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.67962076   0.45256708  13.97225684  -6.86766965  -1.19257435]\n",
      " [  7.31949784 -10.37987417  11.9048566   17.2842641    9.00847378]\n",
      " [  1.           1.           1.           0.           0.        ]]\n",
      "\n",
      "\n",
      "\n",
      "[[-6.86766965 13.97225684]\n",
      " [17.2842641  11.9048566 ]]\n",
      "[[0. 1.]]\n",
      "\n",
      "\n",
      "[[ -1.19257435   0.45256708]\n",
      " [  9.00847378 -10.37987417]]\n",
      "[[0. 1.]]\n",
      "\n",
      "\n",
      "[[4.67962076]\n",
      " [7.31949784]]\n",
      "[[1.]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(2, 5) * 10\n",
    "y = np.array([[1, 1, 1, 0, 0]])\n",
    "\n",
    "print(np.vstack((X, y)))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "NN = L_layer_NN([X.shape[1], 5, 1], batch_size = 2)\n",
    "mini_batches = NN._suffle_into_mini_batches(X, y)\n",
    "\n",
    "for batch in mini_batches:\n",
    "    print(batch[:X.shape[0], :])\n",
    "    print(batch[X.shape[0]:, :])\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bccf17-0559-4d70-992f-0a6fcd9b4bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
